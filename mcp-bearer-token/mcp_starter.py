import asyncio
from typing import Annotated
import os
from dotenv import load_dotenv
from fastmcp import FastMCP
from fastmcp.server.auth.providers.bearer import BearerAuthProvider, RSAKeyPair
from mcp import ErrorData, McpError
from mcp.server.auth.provider import AccessToken
from mcp.types import TextContent, ImageContent, INVALID_PARAMS, INTERNAL_ERROR
from pydantic import BaseModel, Field, AnyUrl

import markdownify
import httpx
import readabilipy
import base64
import json
from datetime import datetime, timedelta

# --- Load environment variables ---
load_dotenv()

TOKEN = os.environ.get("AUTH_TOKEN")
MY_NUMBER = os.environ.get("MY_NUMBER")

assert TOKEN is not None, "Please set AUTH_TOKEN in your .env file"
assert MY_NUMBER is not None, "Please set MY_NUMBER in your .env file"

# --- Auth Provider ---
class SimpleBearerAuthProvider(BearerAuthProvider):
    def __init__(self, token: str):
        k = RSAKeyPair.generate()
        super().__init__(public_key=k.public_key, jwks_uri=None, issuer=None, audience=None)
        self.token = token

    async def load_access_token(self, token: str) -> AccessToken | None:
        if token == self.token:
            return AccessToken(
                token=token,
                client_id="puch-client",
                scopes=["*"],
                expires_at=None,
            )
        return None

# --- Rich Tool Description model ---
class RichToolDescription(BaseModel):
    description: str
    use_when: str
    side_effects: str | None = None

# --- Fetch Utility Class ---
class Fetch:
    USER_AGENT = "Puch/1.0 (Autonomous)"

    @classmethod
    async def fetch_url(
        cls,
        url: str,
        user_agent: str,
        force_raw: bool = False,
    ) -> tuple[str, str]:
        async with httpx.AsyncClient() as client:
            try:
                response = await client.get(
                    url,
                    follow_redirects=True,
                    headers={"User-Agent": user_agent},
                    timeout=30,
                )
            except httpx.HTTPError as e:
                raise McpError(ErrorData(code=INTERNAL_ERROR, message=f"Failed to fetch {url}: {e!r}"))

            if response.status_code >= 400:
                raise McpError(ErrorData(code=INTERNAL_ERROR, message=f"Failed to fetch {url} - status code {response.status_code}"))

            page_raw = response.text

        content_type = response.headers.get("content-type", "")
        is_page_html = "text/html" in content_type

        if is_page_html and not force_raw:
            return cls.extract_content_from_html(page_raw), ""

        return (
            page_raw,
            f"Content type {content_type} cannot be simplified to markdown, but here is the raw content:\n",
        )

    @staticmethod
    def extract_content_from_html(html: str) -> str:
        """Extract and convert HTML content to Markdown format."""
        ret = readabilipy.simple_json.simple_json_from_html_string(html, use_readability=True)
        if not ret or not ret.get("content"):
            return "<error>Page failed to be simplified from HTML</error>"
        content = markdownify.markdownify(ret["content"], heading_style=markdownify.ATX)
        return content

    @staticmethod
    async def google_search_links(query: str, num_results: int = 5) -> list[str]:
        """
        Perform a scoped DuckDuckGo search and return a list of job posting URLs.
        (Using DuckDuckGo because Google blocks most programmatic scraping.)
        """
        ddg_url = f"https://html.duckduckgo.com/html/?q={query.replace(' ', '+')}"
        links = []

        async with httpx.AsyncClient() as client:
            resp = await client.get(ddg_url, headers={"User-Agent": Fetch.USER_AGENT})
            if resp.status_code != 200:
                return ["<error>Failed to perform search.</error>"]

        from bs4 import BeautifulSoup
        soup = BeautifulSoup(resp.text, "html.parser")
        for a in soup.find_all("a", class_="result__a", href=True):
            href = a["href"]
            if "http" in href:
                links.append(href)
            if len(links) >= num_results:
                break

        return links or ["<error>No results found.</error>"]

# --- MCP Server Setup ---
mcp = FastMCP(
    "Job Finder MCP Server",
    auth=SimpleBearerAuthProvider(TOKEN),
)

# --- Tool: validate (required by Puch) ---
@mcp.tool
async def validate() -> str:
    return MY_NUMBER

# --- Tool: job_finder (now smart!) ---
JobFinderDescription = RichToolDescription(
    description="Smart job tool: analyze descriptions, fetch URLs, or search jobs based on free text.",
    use_when="Use this to evaluate job descriptions or search for jobs using freeform goals.",
    side_effects="Returns insights, fetched job descriptions, or relevant job links.",
)

@mcp.tool(description=JobFinderDescription.model_dump_json())
async def job_finder(
    user_goal: Annotated[str, Field(description="The user's goal (can be a description, intent, or freeform query)")],
    job_description: Annotated[str | None, Field(description="Full job description text, if available.")] = None,
    job_url: Annotated[AnyUrl | None, Field(description="A URL to fetch a job description from.")] = None,
    raw: Annotated[bool, Field(description="Return raw HTML content if True")] = False,
) -> str:
    """
    Handles multiple job discovery methods: direct description, URL fetch, or freeform search query.
    """
    if job_description:
        return (
            f"ğŸ“ **Job Description Analysis**\n\n"
            f"---\n{job_description.strip()}\n---\n\n"
            f"User Goal: **{user_goal}**\n\n"
            f"ğŸ’¡ Suggestions:\n- Tailor your resume.\n- Evaluate skill match.\n- Consider applying if relevant."
        )

    if job_url:
        content, _ = await Fetch.fetch_url(str(job_url), Fetch.USER_AGENT, force_raw=raw)
        return (
            f"ğŸ”— **Fetched Job Posting from URL**: {job_url}\n\n"
            f"---\n{content.strip()}\n---\n\n"
            f"User Goal: **{user_goal}**"
        )

    if "look for" in user_goal.lower() or "find" in user_goal.lower():
        links = await Fetch.google_search_links(user_goal)
        return (
            f"ğŸ” **Search Results for**: _{user_goal}_\n\n" +
            "\n".join(f"- {link}" for link in links)
        )

    raise McpError(ErrorData(code=INVALID_PARAMS, message="Please provide either a job description, a job URL, or a search query in user_goal."))


# Image inputs and sending images

MAKE_IMG_BLACK_AND_WHITE_DESCRIPTION = RichToolDescription(
    description="Convert an image to black and white and save it.",
    use_when="Use this tool when the user provides an image URL and requests it to be converted to black and white.",
    side_effects="The image will be processed and saved in a black and white format.",
)

@mcp.tool(description=MAKE_IMG_BLACK_AND_WHITE_DESCRIPTION.model_dump_json())
async def make_img_black_and_white(
    puch_image_data: Annotated[str, Field(description="Base64-encoded image data to convert to black and white")] = None,
) -> list[TextContent | ImageContent]:
    import base64
    import io

    from PIL import Image

    try:
        image_bytes = base64.b64decode(puch_image_data)
        image = Image.open(io.BytesIO(image_bytes))

        bw_image = image.convert("L")

        buf = io.BytesIO()
        bw_image.save(buf, format="PNG")
        bw_bytes = buf.getvalue()
        bw_base64 = base64.b64encode(bw_bytes).decode("utf-8")

        return [ImageContent(type="image", mimeType="image/png", data=bw_base64)]
    except Exception as e:
        raise McpError(ErrorData(code=INTERNAL_ERROR, message=str(e)))

# --- Email Tools ---

EMAIL_SUMMARIZER_DESCRIPTION = RichToolDescription(
    description="Summarize emails and answer questions about email content.",
    use_when="Use this tool when user provides email content and wants summary or has questions about it.",
    side_effects="Analyzes email content and provides structured summary or answers specific questions.",
)

@mcp.tool(description=EMAIL_SUMMARIZER_DESCRIPTION.model_dump_json())
async def email_analyzer(
    email_content: Annotated[str, Field(description="The complete email content including headers, body, and any metadata")],
    analysis_type: Annotated[str, Field(description="Type of analysis: 'summarize', 'action_items', 'sentiment', 'reply_suggestions', 'question'")] = "summarize",
    specific_question: Annotated[str | None, Field(description="Specific question about the email (only used when analysis_type is 'question')")] = None,
) -> str:
    """
    Analyze email content and provide summaries, action items, sentiment analysis, or answer questions.
    """
    if not email_content or len(email_content.strip()) < 10:
        raise McpError(ErrorData(code=INVALID_PARAMS, message="Please provide valid email content."))

    # Extract basic email components
    lines = email_content.strip().split('\n')
    
    # Try to identify email structure
    subject = ""
    sender = ""
    body_start = 0
    
    for i, line in enumerate(lines[:20]):  # Check first 20 lines for headers
        line_lower = line.lower().strip()
        if line_lower.startswith('subject:'):
            subject = line[8:].strip()
        elif line_lower.startswith('from:'):
            sender = line[5:].strip()
        elif line.strip() == "" and i > 0:  # Empty line usually marks start of body
            body_start = i + 1
            break
    
    # Extract body content
    body_lines = lines[body_start:] if body_start > 0 else lines
    body = '\n'.join(body_lines).strip()
    
    # Remove excessive whitespace and clean up
    body = ' '.join(body.split())
    
    if analysis_type == "summarize":
        return f"""ğŸ“§ **Email Summary**

**Subject**: {subject or "No subject found"}
**From**: {sender or "Sender not identified"}

**ğŸ“‹ Key Points:**
{_extract_key_points(body)}

**ğŸ’¡ Main Message:**
{_extract_main_message(body)}

**ğŸ“Š Email Stats:**
- Length: {len(body)} characters
- Word count: {len(body.split())} words
- Estimated reading time: {max(1, len(body.split()) // 200)} minute(s)
"""

    elif analysis_type == "action_items":
        return f"""ğŸ“§ **Action Items Analysis**

**Subject**: {subject or "No subject found"}

**âœ… Action Items Identified:**
{_extract_action_items(body)}

**â° Time-Sensitive Items:**
{_extract_time_sensitive_items(body)}

**ğŸ‘¥ People to Follow Up With:**
{_extract_people_to_contact(body)}
"""

    elif analysis_type == "sentiment":
        return f"""ğŸ“§ **Email Sentiment Analysis**

**Subject**: {subject or "No subject found"}

**ğŸ˜Š Overall Tone**: {_analyze_sentiment(body)}

**ğŸ¯ Intent**: {_analyze_intent(body)}

**ğŸš¨ Urgency Level**: {_analyze_urgency(body)}

**ğŸ’¼ Formality Level**: {_analyze_formality(body)}
"""

    elif analysis_type == "reply_suggestions":
        return f"""ğŸ“§ **Reply Suggestions**

**Subject**: {subject or "No subject found"}

**ğŸ’¬ Suggested Responses:**

**Quick Replies:**
{_suggest_quick_replies(body)}

**Detailed Response Template:**
{_suggest_detailed_response(body)}

**ğŸ“ Key Points to Address:**
{_extract_points_to_address(body)}
"""

    elif analysis_type == "question":
        if not specific_question:
            raise McpError(ErrorData(code=INVALID_PARAMS, message="Please provide a specific question when using 'question' analysis type."))
        
        return f"""ğŸ“§ **Question Analysis**

**Your Question**: {specific_question}

**ğŸ“ Answer Based on Email Content:**
{_answer_question_about_email(body, specific_question)}

**ğŸ“‹ Relevant Email Excerpts:**
{_find_relevant_excerpts(body, specific_question)}
"""

    else:
        raise McpError(ErrorData(code=INVALID_PARAMS, message="Invalid analysis_type. Use: 'summarize', 'action_items', 'sentiment', 'reply_suggestions', or 'question'"))

def _extract_key_points(body: str) -> str:
    """Extract key points from email body"""
    # Simple keyword-based extraction
    sentences = body.split('.')
    key_sentences = []
    
    keywords = ['important', 'urgent', 'deadline', 'meeting', 'please', 'required', 'need', 'must', 'asap', 'immediately']
    
    for sentence in sentences[:10]:  # Limit to first 10 sentences
        sentence = sentence.strip()
        if len(sentence) > 20 and any(keyword in sentence.lower() for keyword in keywords):
            key_sentences.append(f"â€¢ {sentence.strip()}")
    
    if not key_sentences:
        # If no keyword matches, take first few meaningful sentences
        for sentence in sentences[:3]:
            sentence = sentence.strip()
            if len(sentence) > 30:
                key_sentences.append(f"â€¢ {sentence}")
    
    return '\n'.join(key_sentences[:5]) or "â€¢ No specific key points identified"

def _extract_main_message(body: str) -> str:
    """Extract main message from email"""
    # Take first substantial paragraph
    paragraphs = [p.strip() for p in body.split('\n\n') if len(p.strip()) > 50]
    
    if paragraphs:
        main_msg = paragraphs[0]
        if len(main_msg) > 200:
            main_msg = main_msg[:200] + "..."
        return main_msg
    
    # Fallback to first 150 characters
    return body[:150] + "..." if len(body) > 150 else body

def _extract_action_items(body: str) -> str:
    """Extract action items from email"""
    action_words = ['please', 'need you to', 'can you', 'would you', 'could you', 'action required', 'next steps', 'todo', 'follow up']
    
    sentences = body.split('.')
    actions = []
    
    for sentence in sentences:
        sentence_lower = sentence.lower().strip()
        if any(word in sentence_lower for word in action_words):
            actions.append(f"â€¢ {sentence.strip()}")
    
    return '\n'.join(actions[:5]) or "â€¢ No specific action items identified"

def _extract_time_sensitive_items(body: str) -> str:
    """Extract time-sensitive information"""
    time_keywords = ['deadline', 'due', 'urgent', 'asap', 'immediately', 'today', 'tomorrow', 'this week', 'by end of']
    
    sentences = body.split('.')
    time_items = []
    
    for sentence in sentences:
        sentence_lower = sentence.lower().strip()
        if any(keyword in sentence_lower for keyword in time_keywords):
            time_items.append(f"â° {sentence.strip()}")
    
    return '\n'.join(time_items[:3]) or "â° No urgent deadlines identified"

def _extract_people_to_contact(body: str) -> str:
    """Extract people mentioned for follow-up"""
    # Simple email extraction
    import re
    emails = re.findall(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', body)
    
    people = []
    if emails:
        for email in emails[:3]:
            people.append(f"ğŸ‘¤ {email}")
    
    # Look for name patterns
    name_patterns = re.findall(r'\b[A-Z][a-z]+\s+[A-Z][a-z]+\b', body)
    for name in name_patterns[:2]:
        people.append(f"ğŸ‘¤ {name}")
    
    return '\n'.join(people[:5]) or "ğŸ‘¤ No specific contacts identified"

def _analyze_sentiment(body: str) -> str:
    """Simple sentiment analysis"""
    positive_words = ['thank', 'great', 'excellent', 'good', 'appreciate', 'wonderful', 'fantastic']
    negative_words = ['problem', 'issue', 'concern', 'urgent', 'wrong', 'error', 'disappointed']
    
    body_lower = body.lower()
    positive_count = sum(1 for word in positive_words if word in body_lower)
    negative_count = sum(1 for word in negative_words if word in body_lower)
    
    if positive_count > negative_count:
        return "Positive/Friendly"
    elif negative_count > positive_count:
        return "Concerned/Negative"
    else:
        return "Neutral/Professional"

def _analyze_intent(body: str) -> str:
    """Analyze email intent"""
    body_lower = body.lower()
    
    if any(word in body_lower for word in ['meeting', 'schedule', 'calendar']):
        return "Meeting/Scheduling"
    elif any(word in body_lower for word in ['question', 'help', 'clarification']):
        return "Information Request"
    elif any(word in body_lower for word in ['update', 'status', 'progress']):
        return "Status Update"
    elif any(word in body_lower for word in ['please', 'need', 'request']):
        return "Action Request"
    else:
        return "General Communication"

def _analyze_urgency(body: str) -> str:
    """Analyze urgency level"""
    urgent_words = ['urgent', 'asap', 'immediately', 'critical', 'emergency']
    
    if any(word in body.lower() for word in urgent_words):
        return "High - Immediate attention needed"
    elif any(word in body.lower() for word in ['soon', 'quick', 'deadline']):
        return "Medium - Timely response needed"
    else:
        return "Low - No rush"

def _analyze_formality(body: str) -> str:
    """Analyze formality level"""
    formal_indicators = ['dear', 'sincerely', 'regards', 'respectfully']
    informal_indicators = ['hey', 'hi', 'thanks', 'cheers']
    
    body_lower = body.lower()
    
    if any(word in body_lower for word in formal_indicators):
        return "Formal"
    elif any(word in body_lower for word in informal_indicators):
        return "Informal/Casual"
    else:
        return "Semi-formal"

def _suggest_quick_replies(body: str) -> str:
    """Suggest quick reply options"""
    body_lower = body.lower()
    
    replies = []
    
    if 'meeting' in body_lower:
        replies.append("â€¢ 'I'm available for the meeting. Please share the agenda.'")
        replies.append("â€¢ 'Let me check my calendar and get back to you.'")
    
    if any(word in body_lower for word in ['question', 'help']):
        replies.append("â€¢ 'Happy to help! Let me look into this and respond shortly.'")
        replies.append("â€¢ 'Thanks for reaching out. I'll get back to you with details.'")
    
    if 'thank' in body_lower:
        replies.append("â€¢ 'You're welcome! Let me know if you need anything else.'")
    
    if not replies:
        replies = [
            "â€¢ 'Thank you for your email. I'll review and respond accordingly.'",
            "â€¢ 'Received your message. I'll get back to you soon.'",
            "â€¢ 'Thanks for the update. Let me know if you need anything.'"
        ]
    
    return '\n'.join(replies[:3])

def _suggest_detailed_response(body: str) -> str:
    """Suggest detailed response structure"""
    return """**Suggested Response Structure:**

1. **Acknowledgment**: Thank them for their email
2. **Address Main Points**: Respond to key questions/requests
3. **Action Items**: Clarify what you'll do next
4. **Timeline**: When they can expect follow-up
5. **Closing**: Professional sign-off"""

def _extract_points_to_address(body: str) -> str:
    """Extract key points that need to be addressed in reply"""
    # Look for question marks and action words
    sentences = body.split('.')
    points = []
    
    for sentence in sentences:
        if '?' in sentence or any(word in sentence.lower() for word in ['please', 'need', 'can you']):
            points.append(f"â€¢ {sentence.strip()}")
    
    return '\n'.join(points[:4]) or "â€¢ Acknowledge receipt and provide requested information"

def _answer_question_about_email(body: str, question: str) -> str:
    """Answer specific question about email content"""
    question_lower = question.lower()
    body_lower = body.lower()
    
    # Simple keyword matching approach
    if 'who' in question_lower:
        # Look for names/emails
        import re
        emails = re.findall(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', body)
        names = re.findall(r'\b[A-Z][a-z]+\s+[A-Z][a-z]+\b', body)
        
        if emails or names:
            return f"People mentioned: {', '.join(emails + names)}"
    
    elif 'when' in question_lower:
        # Look for dates/times
        import re
        dates = re.findall(r'\b\d{1,2}/\d{1,2}/\d{2,4}\b|\b\d{1,2}-\d{1,2}-\d{2,4}\b', body)
        times = re.findall(r'\b\d{1,2}:\d{2}\b', body)
        
        if dates or times:
            return f"Times/dates mentioned: {', '.join(dates + times)}"
    
    elif 'what' in question_lower:
        # Extract main topic
        return f"Main topic appears to be: {_extract_main_message(body)[:100]}..."
    
    # Fallback: find sentences containing question keywords
    question_words = question_lower.split()
    relevant_sentences = []
    
    for sentence in body.split('.'):
        if any(word in sentence.lower() for word in question_words):
            relevant_sentences.append(sentence.strip())
    
    if relevant_sentences:
        return f"Based on the email content: {' '.join(relevant_sentences[:2])}"
    
    return "I couldn't find specific information related to your question in the email content."

def _find_relevant_excerpts(body: str, question: str) -> str:
    """Find relevant excerpts from email based on question"""
    question_words = question.lower().split()
    sentences = body.split('.')
    
    relevant = []
    for sentence in sentences:
        sentence = sentence.strip()
        if len(sentence) > 20 and any(word in sentence.lower() for word in question_words):
            relevant.append(f"â€¢ \"{sentence}\"")
    
    return '\n'.join(relevant[:3]) or "â€¢ No directly relevant excerpts found"

# --- Gmail Integration Tools ---

GMAIL_OAUTH_DESCRIPTION = RichToolDescription(
    description="Start Gmail OAuth authentication flow to securely connect your Gmail account.",
    use_when="When user wants to connect their Gmail account for the first time or refresh expired access.",
    side_effects="Generates OAuth URL for browser authentication and provides access token retrieval instructions.",
)

@mcp.tool(description=GMAIL_OAUTH_DESCRIPTION.model_dump_json())
async def start_gmail_oauth(
    client_id: Annotated[str | None, Field(description="Your Google OAuth Client ID (optional if using default)")] = None,
) -> str:
    """
    Start Gmail OAuth authentication process with proper browser consent flow.
    """
    
    # Default OAuth client (for demo/testing - users should use their own)
    default_client_id = "YOUR_CLIENT_ID.apps.googleusercontent.com"
    oauth_client_id = client_id or default_client_id
    
    # OAuth scopes - read-only Gmail access
    scopes = "https://www.googleapis.com/auth/gmail.readonly"
    redirect_uri = "https://developers.google.com/oauthplayground"
    
    # Generate OAuth URL
    oauth_url = f"""https://accounts.google.com/o/oauth2/v2/auth?client_id={oauth_client_id}&redirect_uri={redirect_uri}&scope={scopes}&response_type=code&access_type=offline&prompt=consent"""
    
    return f"""ğŸ” **Gmail OAuth Authentication Setup**

## ğŸš€ **Step 1: Create Google OAuth Credentials**
1. Go to: **https://console.cloud.google.com/apis/credentials**
2. Create a new project or select existing one
3. Click **"+ CREATE CREDENTIALS"** â†’ **"OAuth 2.0 Client IDs"**
4. Choose **"Web application"**
5. Add redirect URI: `https://developers.google.com/oauthplayground`
6. Copy your **Client ID** and **Client Secret**

## ğŸ”“ **Step 2: Quick Start (Using OAuth Playground)**
1. Go to: **https://developers.google.com/oauthplayground/**
2. Click the **âš™ï¸ gear icon** (top right)
3. Check **"Use your own OAuth credentials"**
4. Enter your **Client ID** and **Client Secret**
5. In "Step 1", select **"Gmail API v1"** â†’ **"https://www.googleapis.com/auth/gmail.readonly"**
6. Click **"Authorize APIs"** â†’ Sign in and give consent
7. In "Step 2", click **"Exchange authorization code for tokens"**
8. Copy the **"Access token"** and **"Refresh token"**

## ğŸ”’ **Step 3: Direct OAuth URL** (Alternative)
Click this link to start authentication:
{oauth_url}

## âœ… **After Getting Tokens:**
Use `gmail_search_and_analyze` with your access token!

**Example:**
```
search_query: "is:unread -category:promotions -from:noreply"
analysis_type: "summarize"
gmail_access_token: "ya29.a0AfH6SMC..."
max_emails: 50
```

## ğŸ›¡ï¸ **Security Features:**
- âœ… Read-only access only
- âœ… User consent required
- âœ… Automatic promotion/bank filtering
- âœ… Tokens stored locally only
- âœ… No email caching on servers
"""

GMAIL_SEARCH_DESCRIPTION = RichToolDescription(
    description="Search and analyze emails directly from Gmail with smart filtering (no promotions/bank emails).",
    use_when="When user wants to search, summarize, or ask questions about their emails directly from Gmail.",
    side_effects="Connects to Gmail API to fetch and analyze email content with intelligent filtering.",
)

@mcp.tool(description=GMAIL_SEARCH_DESCRIPTION.model_dump_json())
async def gmail_search_and_analyze(
    search_query: Annotated[str, Field(description="Gmail search query (e.g., 'from:boss@company.com', 'subject:meeting', 'is:unread', 'after:2024-01-01')")],
    analysis_type: Annotated[str, Field(description="Type of analysis: 'summarize', 'action_items', 'sentiment', 'reply_suggestions', 'question'")] = "summarize",
    specific_question: Annotated[str | None, Field(description="Specific question about the emails (only used when analysis_type is 'question')")] = None,
    max_emails: Annotated[int, Field(description="Maximum number of emails to analyze (1-100, default: 50)")] = 50,
    gmail_access_token: Annotated[str | None, Field(description="Gmail OAuth2 access token from authentication flow")] = None,
    include_promotions: Annotated[bool, Field(description="Include promotional emails (default: False)")] = False,
    include_bank_emails: Annotated[bool, Field(description="Include bank/financial emails (default: False)")] = False,
) -> str:
    """
    Search Gmail directly and analyze emails with smart filtering and caching.
    """
    
    if not gmail_access_token:
        return """ğŸ” **Gmail Authentication Required**

You need to authenticate with Gmail first to search your emails.

**Quick Setup:**
1. Use the `start_gmail_oauth` tool to get authentication instructions
2. Get your access token from OAuth flow
3. Come back and use this tool with your token

**Example after authentication:**
```
search_query: "is:unread"
analysis_type: "summarize"
gmail_access_token: "ya29.a0AfH6SMC..."
max_emails: 50
```

**Need help?** Use `start_gmail_oauth` tool for step-by-step setup! ğŸš€
"""

    try:
        # Validate max_emails (increased limit as requested)
        max_emails = max(1, min(max_emails, 100))
        
        # Apply smart filtering to search query
        filtered_query = _apply_smart_filtering(search_query, include_promotions, include_bank_emails)
        
        # Check cache first
        cache_key = f"{filtered_query}_{max_emails}_{analysis_type}"
        cached_result = await _get_cached_result(cache_key)
        
        if cached_result:
            return f"""ğŸ“§ **Gmail Search Results** (ğŸ“¦ *cached - faster results*)

{cached_result}

*ï¸âƒ£ Cache expires in 30 minutes for fresh data*
"""
        
        # Search Gmail
        emails = await _search_gmail_emails(gmail_access_token, filtered_query, max_emails)
        
        if not emails:
            return f"""ğŸ“­ **No emails found** for search query: `{filtered_query}`

**Suggestions:**
- Try broader search terms
- Check date ranges (`after:2024-08-01`)  
- Remove specific filters
- Use `is:unread` for recent emails

**Applied Filters:**
- âŒ Promotions excluded: {not include_promotions}
- âŒ Bank emails excluded: {not include_bank_emails}
"""
        
        # Analyze the emails
        result = await _analyze_gmail_results(emails, analysis_type, specific_question, filtered_query)
        
        # Cache the result for 30 minutes
        await _cache_result(cache_key, result, expires_minutes=30)
        
        return result
            
    except Exception as e:
        error_msg = str(e)
        
        # Better error handling
        if "401" in error_msg or "unauthorized" in error_msg.lower():
            return f"""âŒ **Authentication Error**

Your Gmail access token has expired or is invalid.

**Solutions:**
1. Get a fresh token using `start_gmail_oauth` tool
2. Make sure you copied the complete token
3. Check if token has proper Gmail API permissions

**Original Error:** {error_msg}
"""
        elif "403" in error_msg or "forbidden" in error_msg.lower():
            return f"""âŒ **Permission Error**

Gmail API access is restricted.

**Solutions:**
1. Enable Gmail API in Google Cloud Console
2. Check OAuth scope includes gmail.readonly
3. Verify project billing is enabled

**Original Error:** {error_msg}
"""
        else:
            return f"""âŒ **Gmail API Error**

Something went wrong accessing Gmail.

**Error Details:** {error_msg}

**Common Fixes:**
- Check internet connection
- Verify access token format
- Try reducing max_emails parameter
- Use `start_gmail_oauth` for fresh authentication
"""

def _apply_smart_filtering(base_query: str, include_promotions: bool, include_bank_emails: bool) -> str:
    """Apply smart filtering to exclude promotions and bank emails"""
    
    filters = []
    
    # Base query
    filtered_query = base_query.strip()
    
    # Exclude promotions (Gmail's automatic categorization)
    if not include_promotions:
        filters.append("-category:promotions")
        filters.append("-category:social")
        # Common promotional indicators
        filters.append("-from:noreply")
        filters.append("-from:no-reply")
        filters.append("-subject:unsubscribe")
        filters.append("-subject:newsletter")
    
    # Exclude bank/financial emails
    if not include_bank_emails:
        bank_domains = [
            "bank", "banking", "sbi", "hdfc", "icici", "axis", "kotak", "pnb", 
            "paytm", "phonepe", "gpay", "razorpay", "upi", "cred", "jupiter",
            "noreply", "alerts", "notification"
        ]
        
        for domain in bank_domains:
            filters.append(f"-from:{domain}")
        
        # Common bank-related subjects
        bank_subjects = ["statement", "transaction", "payment", "otp", "alert", "debit", "credit"]
        for subject in bank_subjects:
            filters.append(f"-subject:{subject}")
    
    # Combine base query with filters
    if filters:
        filtered_query += " " + " ".join(filters)
    
    return filtered_query

# Simple in-memory cache (in production, use Redis or similar)
_email_cache = {}

async def _get_cached_result(cache_key: str) -> str | None:
    """Get cached email analysis result"""
    
    if cache_key in _email_cache:
        cached_data = _email_cache[cache_key]
        
        # Check if cache is expired (30 minutes)
        if datetime.now() < cached_data['expires']:
            return cached_data['result']
        else:
            # Remove expired cache
            del _email_cache[cache_key]
    
    return None

async def _cache_result(cache_key: str, result: str, expires_minutes: int = 30):
    """Cache email analysis result"""
    
    _email_cache[cache_key] = {
        'result': result,
        'expires': datetime.now() + timedelta(minutes=expires_minutes),
        'created': datetime.now()
    }
    
    # Clean up old cache entries (keep max 100 entries)
    if len(_email_cache) > 100:
        # Remove oldest entries
        sorted_keys = sorted(_email_cache.keys(), key=lambda k: _email_cache[k]['created'])
        for key in sorted_keys[:20]:  # Remove 20 oldest
            del _email_cache[key]

async def _analyze_gmail_results(emails: list[dict], analysis_type: str, specific_question: str | None, search_query: str) -> str:
    """Analyze Gmail search results"""
    
    email_summaries = []
    
    for i, email in enumerate(emails, 1):
        subject = email.get('subject', 'No Subject')
        sender = email.get('sender', 'Unknown Sender')
        date = email.get('date', 'Unknown Date')
        
        # Clean sender (remove extra info)
        sender = sender.split('<')[0].strip() if '<' in sender else sender
        
        email_summaries.append(f"**{i}.** {subject[:60]}{'...' if len(subject) > 60 else ''}\n   ğŸ“§ {sender} â€¢ ğŸ“… {date[:16]}")
    
    # Perform analysis based on type
    if analysis_type == "summarize":
        return f"""ğŸ“§ **Gmail Search Results & Summary**

**ğŸ” Search Query:** `{search_query}`
**ğŸ“Š Found:** {len(emails)} email(s) (filtered: no promotions/bank emails)

**ğŸ“‹ Email List:**
{chr(10).join(email_summaries)}

**ğŸ” Overall Analysis:**
{_analyze_multiple_emails(emails, analysis_type)}

**âš¡ Smart Features Applied:**
- âœ… Promotional emails filtered out
- âœ… Bank/financial alerts excluded  
- âœ… Results cached for 30 minutes
- âœ… Privacy-focused (no server storage)
"""
    
    elif analysis_type == "action_items":
        return f"""ğŸ“§ **Action Items from Gmail Search**

**ğŸ” Search Query:** `{search_query}`
**ğŸ“Š Emails Analyzed:** {len(emails)}

**âœ… Action Items Found:**
{_extract_action_items_from_multiple(emails)}

**â° Urgent Items:**
{_extract_urgent_items_from_multiple(emails)}

**ğŸ“§ Source Emails:**
{chr(10).join(email_summaries[:5])}
"""
    
    elif analysis_type == "question":
        if not specific_question:
            raise McpError(ErrorData(code=INVALID_PARAMS, message="Please provide a specific question when using 'question' analysis type."))
        
        return f"""ğŸ“§ **Question Analysis - Gmail Search**

**ğŸ” Search Query:** `{search_query}`
**â“ Your Question:** {specific_question}
**ğŸ“Š Emails Analyzed:** {len(emails)}

**ğŸ“ Answer:**
{_answer_question_about_multiple_emails(emails, specific_question)}

**ğŸ“‹ Relevant Email Excerpts:**
{_find_relevant_excerpts_from_multiple(emails, specific_question)}

**ğŸ“§ Source Emails:**
{chr(10).join(email_summaries[:3])}
"""
    
    else:
        # For sentiment and reply_suggestions, analyze first few emails
        results = []
        for i, email in enumerate(emails[:3], 1):  # Limit to first 3 for detailed analysis
            email_analysis = await email_analyzer(
                email_content=f"Subject: {email.get('subject', '')}\nFrom: {email.get('sender', '')}\n\n{email.get('body', '')}",
                analysis_type=analysis_type,
                specific_question=specific_question
            )
            results.append(f"**Email {i} Analysis:**\n{email_analysis}\n")
        
        return f"""ğŸ“§ **Gmail Search & {analysis_type.title()} Analysis**

**ğŸ” Search Query:** `{search_query}`
**ğŸ“Š Emails Found:** {len(emails)} (showing detailed analysis for first 3)

{chr(10).join(results)}

**ğŸ“§ All Emails Found:**
{chr(10).join(email_summaries)}
"""

async def _search_gmail_emails(access_token: str, search_query: str, max_results: int = 50) -> list[dict]:
    """Search Gmail and return email data"""
    
    async with httpx.AsyncClient() as client:
        # Step 1: Search for message IDs
        search_url = f"https://gmail.googleapis.com/gmail/v1/users/me/messages"
        
        headers = {
            "Authorization": f"Bearer {access_token}",
            "Content-Type": "application/json"
        }
        
        params = {
            "q": search_query,
            "maxResults": max_results
        }
        
        response = await client.get(search_url, headers=headers, params=params)
        
        if response.status_code != 200:
            raise McpError(ErrorData(code=INTERNAL_ERROR, message=f"Gmail search failed: {response.status_code} - {response.text}"))
        
        search_results = response.json()
        messages = search_results.get('messages', [])
        
        if not messages:
            return []
        
        # Step 2: Fetch full message details
        emails = []
        for message in messages:
            message_id = message['id']
            
            message_url = f"https://gmail.googleapis.com/gmail/v1/users/me/messages/{message_id}"
            message_response = await client.get(message_url, headers=headers)
            
            if message_response.status_code == 200:
                email_data = message_response.json()
                parsed_email = _parse_gmail_message(email_data)
                emails.append(parsed_email)
        
        return emails

def _parse_gmail_message(gmail_message: dict) -> dict:
    """Parse Gmail API message into readable format"""
    
    payload = gmail_message.get('payload', {})
    headers = payload.get('headers', [])
    
    # Extract headers
    subject = ""
    sender = ""
    date = ""
    
    for header in headers:
        name = header.get('name', '').lower()
        value = header.get('value', '')
        
        if name == 'subject':
            subject = value
        elif name == 'from':
            sender = value
        elif name == 'date':
            date = value
    
    # Extract body
    body = _extract_gmail_body(payload)
    
    return {
        'subject': subject,
        'sender': sender,
        'date': date,
        'body': body,
        'message_id': gmail_message.get('id', '')
    }

def _extract_gmail_body(payload: dict) -> str:
    """Extract email body from Gmail payload"""
    
    body = ""
    
    # Check if it's a simple message
    if 'body' in payload and payload['body'].get('data'):
        body = base64.urlsafe_b64decode(payload['body']['data']).decode('utf-8', errors='ignore')
    
    # Check for multipart messages
    elif 'parts' in payload:
        for part in payload['parts']:
            if part.get('mimeType') == 'text/plain' and part.get('body', {}).get('data'):
                part_body = base64.urlsafe_b64decode(part['body']['data']).decode('utf-8', errors='ignore')
                body += part_body + "\n"
            elif part.get('mimeType') == 'text/html' and part.get('body', {}).get('data') and not body:
                # Fallback to HTML if no plain text
                html_body = base64.urlsafe_b64decode(part['body']['data']).decode('utf-8', errors='ignore')
                # Simple HTML to text conversion
                import re
                body = re.sub('<[^<]+?>', '', html_body)
    
    return body.strip()

def _analyze_multiple_emails(emails: list[dict], analysis_type: str) -> str:
    """Analyze multiple emails and provide summary"""
    
    total_emails = len(emails)
    
    if analysis_type == "summarize":
        # Count key metrics
        senders = set()
        subjects = []
        total_length = 0
        
        for email in emails:
            senders.add(email.get('sender', 'Unknown'))
            subjects.append(email.get('subject', 'No Subject'))
            total_length += len(email.get('body', ''))
        
        return f"""**ğŸ“Š Email Summary:**
â€¢ Total emails: {total_emails}
â€¢ Unique senders: {len(senders)}
â€¢ Average email length: {total_length // total_emails if total_emails > 0 else 0} characters

**ğŸ‘¥ Top Senders:**
{chr(10).join([f"â€¢ {sender}" for sender in list(senders)[:5]])}

**ğŸ“ Recent Subjects:**
{chr(10).join([f"â€¢ {subject}" for subject in subjects[:5]])}
"""

def _extract_action_items_from_multiple(emails: list[dict]) -> str:
    """Extract action items from multiple emails"""
    
    all_actions = []
    
    for i, email in enumerate(emails, 1):
        body = email.get('body', '')
        actions = _extract_action_items(body)
        
        if actions and actions != "â€¢ No specific action items identified":
            all_actions.append(f"**From Email {i} ({email.get('subject', 'No Subject')}):**\n{actions}")
    
    return '\n\n'.join(all_actions) if all_actions else "â€¢ No action items found across emails"

def _extract_urgent_items_from_multiple(emails: list[dict]) -> str:
    """Extract urgent items from multiple emails"""
    
    urgent_items = []
    
    for i, email in enumerate(emails, 1):
        body = email.get('body', '')
        urgent = _extract_time_sensitive_items(body)
        
        if urgent and urgent != "â° No urgent deadlines identified":
            urgent_items.append(f"**Email {i}:** {urgent}")
    
    return '\n\n'.join(urgent_items) if urgent_items else "â° No urgent items found"

def _answer_question_about_multiple_emails(emails: list[dict], question: str) -> str:
    """Answer question about multiple emails"""
    
    # Combine all email content
    combined_content = ""
    for email in emails:
        combined_content += f" {email.get('subject', '')} {email.get('body', '')}"
    
    # Use existing question answering logic
    return _answer_question_about_email(combined_content, question)

def _find_relevant_excerpts_from_multiple(emails: list[dict], question: str) -> str:
    """Find relevant excerpts from multiple emails"""
    
    question_words = question.lower().split()
    relevant_excerpts = []
    
    for i, email in enumerate(emails, 1):
        subject = email.get('subject', '')
        body = email.get('body', '')
        
        # Check subject
        if any(word in subject.lower() for word in question_words):
            relevant_excerpts.append(f"â€¢ **Email {i} Subject:** \"{subject}\"")
        
        # Check body sentences
        sentences = body.split('.')
        for sentence in sentences[:3]:  # First 3 sentences only
            if len(sentence.strip()) > 20 and any(word in sentence.lower() for word in question_words):
                relevant_excerpts.append(f"â€¢ **Email {i}:** \"{sentence.strip()}\"")
                break
    
    return '\n'.join(relevant_excerpts[:5]) if relevant_excerpts else "â€¢ No directly relevant excerpts found"

GMAIL_SETUP_DESCRIPTION = RichToolDescription(
    description="Get step-by-step instructions to connect your Gmail account.",
    use_when="When user wants to set up Gmail integration for the first time.",
    side_effects="Provides OAuth setup instructions and example search queries.",
)

@mcp.tool(description=GMAIL_SETUP_DESCRIPTION.model_dump_json())
async def gmail_setup_guide() -> str:
    """
    Provide step-by-step instructions for Gmail OAuth setup.
    """
    
    return """ğŸ”§ **Gmail Integration Setup Guide**

## ğŸš€ **Quick Setup (5 minutes):**

### **Step 1: Create Google Cloud Project**
1. Go to: **https://console.cloud.google.com/**
2. Create new project or select existing
3. Enable **Gmail API**: Search "Gmail API" â†’ Enable

### **Step 2: Create OAuth Credentials**
1. Go to: **https://console.cloud.google.com/apis/credentials**
2. Click **"+ CREATE CREDENTIALS"** â†’ **"OAuth 2.0 Client IDs"**
3. Choose **"Web application"**
4. Name: "Gmail Email Analyzer"
5. **Authorized redirect URIs:** Add `https://developers.google.com/oauthplayground`
6. Click **"CREATE"** â†’ Copy **Client ID** and **Client Secret**

### **Step 3: Get Access Token**
1. Go to: **https://developers.google.com/oauthplayground/**
2. Click **âš™ï¸ Settings** (top right)
3. Check **"Use your own OAuth credentials"**
4. Paste your **Client ID** and **Client Secret**
5. In "Step 1" â†’ Select **"Gmail API v1"** â†’ **"https://www.googleapis.com/auth/gmail.readonly"**
6. Click **"Authorize APIs"** â†’ **Sign in to Gmail** â†’ **Allow access**
7. In "Step 2" â†’ Click **"Exchange authorization code for tokens"**
8. Copy **"Access token"** and **"Refresh token"**

## ğŸ“§ **Test Your Setup:**

```
ğŸ” Search recent emails (excluding promotions/bank):
search_query: "is:unread"
analysis_type: "summarize"
gmail_access_token: "ya29.a0AfH6SMC..."
max_emails: 50

ğŸ“… Work emails from this week:
search_query: "after:2024-08-05 -category:promotions"
analysis_type: "action_items"

â“ Ask questions about specific emails:
search_query: "from:boss@company.com"
analysis_type: "question"
specific_question: "What are the project deadlines?"
```

## ï¿½ï¸ **Smart Features:**

### **ğŸš« Auto-Filtering:**
- âŒ **Promotions** (newsletters, marketing)
- âŒ **Bank emails** (OTPs, statements, alerts)
- âŒ **Social notifications** (LinkedIn, Facebook)
- âœ… **Only important emails** analyzed

### **âš¡ Performance:**
- ğŸ“¦ **Caching**: Results cached for 30 minutes
- ğŸš€ **Fast searches**: Up to 100 emails
- ğŸ”’ **Privacy**: No emails stored on servers
- ğŸ”„ **Auto-refresh**: Expired tokens handled

### **ğŸ¯ Search Examples:**
```
Subject-based: "subject:meeting OR subject:project"
Sender-based: "from:manager@company.com"
Date-based: "after:2024-08-01 before:2024-08-08"
Combined: "from:hr subject:leave after:2024-08-01"
Unread only: "is:unread -category:promotions"
With attachments: "has:attachment from:colleague"
```

## ğŸ†˜ **Troubleshooting:**

**Token expires in 1 hour?**
- Use **Refresh Token** to get new Access Token
- Or re-run OAuth flow for fresh tokens

**No emails found?**
- Check search syntax
- Try broader terms
- Use `is:unread` for recent emails

**Permission denied?**
- Verify Gmail API is enabled
- Check OAuth scope is correct
- Ensure billing is set up (if required)

Ready to analyze your emails intelligently! ğŸ‰"""

# --- Run MCP Server ---
async def main():
    port = int(os.environ.get("PORT", 10000))
    print(f"ğŸš€ Starting MCP server on http://0.0.0.0:{port}")
    await mcp.run_async("streamable-http", host="0.0.0.0", port=port)

if __name__ == "__main__":
    asyncio.run(main())
